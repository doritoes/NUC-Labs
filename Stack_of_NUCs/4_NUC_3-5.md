# Set up NUC 3, NUC 4, and more
In this step you will configure your remaining NUCs in the stack as more servers, the Ansible member modes. We will update the CIDATA USB stick and use it to build the remaining NUCs in the stack.

Hardware:
- [D34010WYK](https://www.intel.com/content/www/us/en/products/sku/76978/intel-nuc-kit-d34010wyk/specifications.html) - i3
  - https://www.intel.com/content/www/us/en/download/17536/bios-update-wylpt10h.html
  - 8 GB RAM
  - 32 GB or more storage
  - Wireless card

Software:
- Ubuntu 22.04 LTS server

Purpose:
- Ansible worker nodes

## Modify the CIDATA USB stick for the Ansible Nodes
These steps are performed while logged in to NUC1
- Insert the CICDATA USB stick into NUC1
  - The USB stick will mount automatically
- Modify the `user-data` file on CIDATA (the USB stick)
  - Download the example file: [user-data-ansible](user-data-ansible-node)
  - Replace the contents on `user-data` on the USB stick with the contents of the example file
    - ⚠️ Replace the WiFi SSID name and PASSWORD with your WiFi SSID and passphrase
    - ⚠️ Replace the key(s) in the example for the user **tux** with the output from **NUC1** for `cat ~/.ssh/id_rsa.pub`
    - ⚠️ Replace the key(s) in the example for the user **ansible** with the output from **NUC2** (logged in as user ansible) for `cat ~/.ssh/id_rsa.pub`
- Safely eject the USB stick ([tip](Appendix_Safely_Eject.md))

## Build Ansible Nodes
Repeat this process for the remaining NUCs in the stack.

- make sure the NUC is powered off
- connect
  - connect the ethernet port to your lab network (which should have Internet access)
  - keyboard, video and mouse
- insert both USB sticks
- power on the NUC
- press F10 when prompted and select the USB bootable USB stick (“USB UEFI”)
- wait patiently
  - if the user-data file is configured to update packages, be aware this requires additional time
  - if the ISO is older, it may take a while to download and install all the security updates
- when the NUC powers down the first time, <ins>remove the USB sticks</ins> and power it back on
- when the NUC powers down the second time, it is ready to deploy


## Learn More
### Power
I have a UPS on the other side of the Lab where I plug all my worker NUCs in. Only power is needed . If you  have a large supply of NUCs, you can estimate the power draw to ensure you aren't overloading circuits. Look up the TDP Wattage for your NUCs. For my D34010WYK's, that's only 15W TDP. So you can run a LOT of NUCs. There are a wide variety of "Kill A Watt" type electricity usage monitors available. I use mine frequently.

### Experiment with your NUCs Lab
Here are some things you can experiment with now.
- Test logging in (for now you will need to manually accept the keys; we will add those automatically later)
  - from NUC 1 logged in as tux
    - `ssh tux@[IP NUC3]`
    - `exit back to NUC 1`
    - `ssh ansible@[IP NUC3]`
    - exit back to NUC 1
  - from NUC 2 logged in as ansible
    - `ssh [IP NUC3]`
- Test running ad-hoc Ansible commands from NUC 2 to NUC 3
  - Add the IP address of NUC 3 to the inventory
    - File: /home/ansible/my-project/hosts
    - Under [nodes] add the IP address
  - Run some test Ansible ad-hoc commands
    - `ansible -i hosts all –list-hosts`
    - `ansible -i hosts all -m ping`
    - `ansible -i hosts nodes -m ping`
    - `ansible -i hosts all -a "/sbin/reboot"`
      - in my testing I get a connection refused message as the NUC reboots
      - after it comes back up you can confirm it rebooted by looking at the uptime
      - `ansible -i hosts all -a "/usr/bin/uptime"`
    - `ansible -i hosts all -m ansible.builtin.setup # see all facts`
  - Optionally, take the NUC 3 IP address back out of the inventory file “hosts”
